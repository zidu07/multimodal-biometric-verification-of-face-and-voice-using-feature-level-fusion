{"cells":[{"cell_type":"markdown","metadata":{"id":"NgR0j5310qqC"},"source":["# Face recognition\n","Task is to recognize a faces"]},{"cell_type":"markdown","metadata":{"id":"X_f3HHLmJIuT"},"source":["### Dataset\n","**Aligned Face Dataset from Pinterest**\n","\n","This dataset contains 10.770 images for 100 people. All images are taken from 'Pinterest' and      aligned using dlib library."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fn8OlNXgMarq"},"outputs":[],"source":["%tensorflow_version 2.x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":4939,"status":"ok","timestamp":1651596509499,"user":{"displayName":"test1 code","userId":"14947689959402461756"},"user_tz":-60},"id":"aV3hpS9ciSFr","outputId":"ca051a8e-0c28-4681-e132-156e190da87b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.8.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["import tensorflow\n","tensorflow.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kv6CqWuIjQhW"},"outputs":[],"source":["import zipfile\n","import os\n","\n","os.environ['KAGGLE_CONFIG_DIR'] = \"/content\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1650995157180,"user":{"displayName":"test1 code","userId":"14947689959402461756"},"user_tz":-60},"id":"a44b3oBqjeMM","outputId":"1987554b-5d7f-4f4d-df63-2d93e54e43f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/bin/kaggle\", line 5, in <module>\n","    from kaggle.cli import main\n","  File \"/usr/local/lib/python3.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n","    api.authenticate()\n","  File \"/usr/local/lib/python3.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 166, in authenticate\n","    self.config_file, self.config_dir))\n","OSError: Could not find kaggle.json. Make sure it's located in /content. Or use the environment method.\n"]}],"source":["!kaggle datasets download -d hereisburak/pins-face-recognition"]},{"cell_type":"markdown","metadata":{"id":"CjRTlPkp1LC2"},"source":["#### Mount Google drive if you are using google colab\n","- We recommend using Google Colab as you can face memory issues and longer runtimes while running on local"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15830,"status":"ok","timestamp":1651591146279,"user":{"displayName":"test1 code","userId":"14947689959402461756"},"user_tz":-60},"id":"sBWMoTJ9cf3Z","outputId":"59e3b078-3bf4-431a-8a6c-2f080aba59d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"sO9mgMmp13sI"},"source":["#### Change current working directory to project folder "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TddMnf4D1-59"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/My Drive/face_recognition/')"]},{"cell_type":"markdown","metadata":{"id":"CBB_OncAQ8h_"},"source":["### Extract the zip file\n","- Extract Aligned Face Dataset from Pinterest.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":910,"status":"error","timestamp":1650995158086,"user":{"displayName":"test1 code","userId":"14947689959402461756"},"user_tz":-60},"id":"_uCdBvpQkFhi","outputId":"07cf1fa4-588f-4b41-d673-298f847a41f6"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-cde62bb1b8e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzip_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/pins-face-recognition.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Opens the zip file in read mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Extracts the files into the /tmp folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/pins-face-recognition.zip'"]}],"source":["zip_ref = zipfile.ZipFile('/content/pins-face-recognition.zip', 'r') #Opens the zip file in read mode\n","zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n","zip_ref.close()"]},{"cell_type":"markdown","metadata":{"id":"oesXJD9ySB6w"},"source":["### Function to load images\n","- Define a function to load the images from the extracted folder and map each image with person id \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1692,"status":"ok","timestamp":1651598315431,"user":{"displayName":"test1 code","userId":"14947689959402461756"},"user_tz":-60},"id":"4Q7TS19vVbGb","outputId":"00e8103f-8abc-4473-9d73-d04ed25f3f3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["[105_classes_pins_dataset/pins_Adriana Lima/Adriana Lima0_0.jpg\n"," 105_classes_pins_dataset/pins_Adriana Lima/Adriana Lima101_3.jpg\n"," 105_classes_pins_dataset/pins_Adriana Lima/Adriana Lima102_4.jpg ...\n"," 105_classes_pins_dataset/pins_Rebecca Ferguson/Rebecca Ferguson222_432.jpg\n"," 105_classes_pins_dataset/pins_Rebecca Ferguson/Rebecca Ferguson223_433.jpg\n"," 105_classes_pins_dataset/pins_Rebecca Ferguson/Rebecca Ferguson224_434.jpg]\n"]}],"source":["import numpy as np\n","import os\n","\n","class IdentityMetadata():\n","    def __init__(self, base, name, file):\n","        # print(base, name, file)\n","        # dataset base directory\n","        self.base = base\n","        # identity name\n","        self.name = name\n","        # image file name\n","        self.file = file\n","\n","    def __repr__(self):\n","        return self.image_path()\n","\n","    def image_path(self):\n","        return os.path.join(self.base, self.name, self.file) \n","    \n","def load_metadata(path):\n","    metadata = []\n","    for i in os.listdir(path):\n","        for f in os.listdir(os.path.join(path, i)):\n","            # Check file extension. Allow only jpg/jpeg' files.\n","            ext = os.path.splitext(f)[1]\n","            if ext == '.jpg' or ext == '.jpeg':\n","                metadata.append(IdentityMetadata(path, i, f))\n","    return np.array(metadata)\n","\n","# metadata = load_metadata('images')\n","metadata = load_metadata('105_classes_pins_dataset')\n","print(metadata)"]},{"cell_type":"markdown","metadata":{"id":"nG1Vzl3MPebA"},"source":["### function to load image\n","-  function to load image from the metadata"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ape5WxvVWKOe"},"outputs":[],"source":["import cv2\n","def load_image(path):\n","    img = cv2.imread(path, 1)\n","    # OpenCV loads images with color channels\n","    # in BGR order. So we need to reverse them\n","    return img[...,::-1]"]},{"cell_type":"markdown","metadata":{"id":"DYm-aYUDRANv"},"source":["#### Load a sample image\n","- Load one image using the function \"load_image\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1651598317916,"user":{"displayName":"test1 code","userId":"14947689959402461756"},"user_tz":-60},"id":"ptDNq8noWK89","outputId":"c0c8e882-f5a2-4e4f-c251-fc1b47fe6fdd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[28, 28, 38],\n","        [28, 28, 38],\n","        [28, 28, 38],\n","        ...,\n","        [29, 35, 49],\n","        [28, 34, 48],\n","        [28, 34, 48]],\n","\n","       [[28, 28, 38],\n","        [28, 28, 38],\n","        [28, 28, 38],\n","        ...,\n","        [29, 35, 49],\n","        [28, 34, 48],\n","        [28, 34, 48]],\n","\n","       [[28, 28, 38],\n","        [28, 28, 38],\n","        [28, 28, 38],\n","        ...,\n","        [30, 36, 50],\n","        [29, 35, 49],\n","        [28, 34, 48]],\n","\n","       ...,\n","\n","       [[11,  9, 12],\n","        [ 7,  5,  8],\n","        [ 5,  3,  6],\n","        ...,\n","        [17, 19, 31],\n","        [17, 19, 31],\n","        [17, 19, 31]],\n","\n","       [[10,  8, 11],\n","        [ 8,  6,  9],\n","        [ 6,  4,  7],\n","        ...,\n","        [17, 20, 29],\n","        [17, 20, 29],\n","        [17, 20, 29]],\n","\n","       [[10,  8, 11],\n","        [ 8,  6,  9],\n","        [ 6,  4,  7],\n","        ...,\n","        [17, 20, 29],\n","        [17, 20, 29],\n","        [17, 20, 29]]], dtype=uint8)"]},"metadata":{},"execution_count":6}],"source":["# Load an image\n","# for example, loading the image with index 1\n","load_image(metadata[0].image_path())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iU-1YPIZXyLv"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"yg0olr-8Xbqw"},"source":["### VGG Face model\n","- Here we have the predefined model for VGG face"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hh0Pz6acuaDP"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\n","\n","def vgg_face():\t\n","    model = Sequential()\n","    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n","    model.add(Convolution2D(64, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(64, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(128, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(128, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(256, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(256, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(256, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Convolution2D(2622, (1, 1)))\n","    model.add(Flatten())\n","    model.add(Activation('softmax'))\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"5YGerjZTGfF6"},"source":["#### Load the model \n","- Load the model defined above\n","- Then load the given weight file named \"vgg_face_weights.h5\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":541,"status":"ok","timestamp":1651591205111,"user":{"displayName":"test1 code","userId":"14947689959402461756"},"user_tz":-60},"id":"jhWowlCSv121","outputId":"8a7daa68-e5df-4dee-91e8-807da45a0e38"},"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/bin/kaggle\", line 5, in <module>\n","    from kaggle.cli import main\n","  File \"/usr/local/lib/python3.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n","    api.authenticate()\n","  File \"/usr/local/lib/python3.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 166, in authenticate\n","    self.config_file, self.config_dir))\n","OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"]}],"source":["!kaggle datasets download -d acharyarupak391/vggfaceweights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fmHpFGs_wIRV"},"outputs":[],"source":["zip_ref = zipfile.ZipFile('vggfaceweights.zip', 'r') #Opens the zip file in read mode\n","zip_ref.extractall('/content/drive/MyDrive/face_recognition/') #Extracts the files into the /tmp folder\n","zip_ref.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zAa3OASPvKac"},"outputs":[],"source":["from tensorflow.keras.models import model_from_json\n","model = vgg_face()\n","model.load_weights('vgg_face_weights.h5')"]},{"cell_type":"markdown","metadata":{"id":"T2WfmWZ5Gq-4"},"source":["### Get vgg_face_descriptor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9IQ9hcSwO9k"},"outputs":[],"source":["from tensorflow.keras.models import Model\n","vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)"]},{"cell_type":"markdown","metadata":{"id":"LkBQRL_sd2U8"},"source":["### Generate embeddings for each image in the dataset\n","- Given below is an example to load the first image in the metadata and get its embedding vector from the pre-trained model. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1580,"status":"ok","timestamp":1651598338326,"user":{"displayName":"test1 code","userId":"14947689959402461756"},"user_tz":-60},"id":"B2yd69OydBAq","outputId":"8b46283d-10e0-48ad-acd7-bba84ba7e6d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["(224, 224, 3)\n","(2622,)\n","[ 0.01242804 -0.00290044  0.01809887 ... -0.04276752  0.0208946\n","  0.0535069 ]\n"]}],"source":["# Get embedding vector for first image in the metadata using the pre-trained model\n","\n","img_path = metadata[0].image_path()\n","img = load_image(img_path)\n","\n","# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\n","img = (img / 255.).astype(np.float32)\n","\n","img = cv2.resize(img, dsize = (224,224))\n","print(img.shape)\n","\n","#img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","\n","# Obtain embedding vector for an image\n","# Get the embedding vector for the above image using vgg_face_descriptor model and print the shape \n","\n","embedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\n","print(embedding_vector.shape)\n","print(embedding_vector)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5kynyBVrJl_"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"plHvUTytcTGo"},"source":["### Generate embeddings for all images\n","- code to iterate through metadata and create embeddings for each image using `vgg_face_descriptor.predict()` and store in a list with name `embeddings`\n","\n","- If there is any error in reading any image in the dataset, the emebdding vector of that image will be 2622-zeroes as the final embedding from the model is of length 2622."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yY9ykxtueY4k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651606554190,"user_tz":-60,"elapsed":5703000,"user":{"displayName":"test1 code","userId":"14947689959402461756"}},"outputId":"dffd4bda-3fcb-49fc-e880-c27e382c28ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["'NoneType' object is not subscriptable\n","6128 105_classes_pins_dataset/pins_Jessica Barden/Jessica Barden245_1470.jpg\n","'NoneType' object is not subscriptable\n","11141 105_classes_pins_dataset/pins_Rebecca Ferguson/Rebecca Ferguson224_434.jpg\n"]}],"source":["embeddings = np.zeros((metadata.shape[0], 2622))\n","\n","for i, m in enumerate(metadata):\n","    try:\n","        img = load_image(m.image_path())\n","        # scale RGB values to interval [0,1]\n","        img = cv2.resize(img, dsize = (224,224))\n","        img = (img / 255.).astype(np.float32)\n","       # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","        # obtain embedding vector for image\n","        embeddings[i] = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\n","        \n","\n","       \n","    except Exception as e:\n","        print(str(e))\n","        print(i,m)\n","        \n","np.save('/content/drive/MyDrive/face_recognition/embed',embeddings)        "]},{"cell_type":"markdown","metadata":{"id":"4hb3XSDsfTMG"},"source":["### Function to calculate distance between given 2 pairs of images.\n","\n","- Consider distance metric as \"Squared L2 distance\"\n","- Squared l2 distance between 2 points (x1, y1) and (x2, y2) = (x1-x2)^2 + (y1-y2)^2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sNnRtt-U7aU"},"outputs":[],"source":["def distance(emb1, emb2):\n","    return np.sum(np.square(emb1 - emb2))"]},{"cell_type":"markdown","metadata":{"id":"JwVRkeoNUyUw"},"source":["#### Plot images and get distance between the pairs given below\n","- 2, 3 and 2, 180\n","- 30, 31 and 30, 100\n","- 70, 72 and 70, 115"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDVLED10eboB"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def show_pair(idx1, idx2):\n","    plt.figure(figsize=(8,3))\n","    plt.suptitle(f'Distance = {distance(embeddings[idx1], embeddings[idx2]):.2f}')\n","    plt.subplot(121)\n","    plt.imshow(load_image(metadata[idx1].image_path()))\n","    plt.subplot(122)\n","    plt.imshow(load_image(metadata[idx2].image_path()));    \n","\n","show_pair(2, 3)\n","show_pair(2, 180)"]},{"cell_type":"markdown","metadata":{"id":"-G2iDeWKYMae"},"source":["### train and test sets\n","-  X_train, X_test and y_train, y_test\n","- Used train_idx to seperate out training features and labels\n","- Used test_idx to seperate out testing features and labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OThdBDPxYkd4"},"outputs":[],"source":["train_idx = np.arange(metadata.shape[0]) % 9 != 0\n","test_idx = np.arange(metadata.shape[0]) % 9 == 0\n","\n","# one half as train examples of 10 identities\n","X_train = embeddings[train_idx]\n","# another half as test examples of 10 identities\n","X_test = embeddings[test_idx]\n","\n","targets = np.array([m.name for m in metadata])\n","y_train = targets[train_idx]\n","y_test = targets[test_idx]"]},{"cell_type":"markdown","metadata":{"id":"DlYYwGQxXVwf"},"source":["### Encode the Labels\n","- Encode the targets\n","- Use LabelEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8GOQrjqeX2LZ"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","encoder = LabelEncoder()\n","\n","# Numerical encoding of identities\n","y_train = encoder.fit_transform(y_train)\n","y_test = encoder.transform(y_test)"]},{"cell_type":"markdown","metadata":{"id":"o9CylOWOa4xM"},"source":["### Standardize the feature values\n","- Scale the features using StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H7pUV0oYbLrR"},"outputs":[],"source":["# Standarize features\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"i2QukHGXbb6d"},"source":["### Reduce dimensions using PCA\n","- Reduce feature dimensions using Principal Component Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dVj1SSEebtG8"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","pca = PCA(n_components=128, svd_solver='randomized', whiten=True)\n","\n","X_train = pca.fit_transform(X_train)\n","X_test = pca.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"SzCsmZg8chW4"},"source":["### Build a Classifier\n","- Use SVM Classifier to predict the person in the given image\n","- Fit the classifier and print the score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnBv9Ks0cwtA"},"outputs":[],"source":["from sklearn.svm import SVC\n","\n","clf = SVC(kernel='rbf', class_weight=None , C=10000000, gamma='auto')\n","clf.fit(X_train, y_train)\n","clf.score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"JGz1G8e3dUl5"},"source":["### Test results\n","- Take 10th image from test set and plot the image\n","- Report to which person(folder name in dataset) the image belongs to"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4zD_f8Sudeiw"},"outputs":[],"source":["import warnings\n","# Suppress LabelEncoder warning\n","warnings.filterwarnings('ignore')\n","\n","example_idx = 30\n","\n","example_image = load_image(metadata[test_idx][example_idx].image_path())\n","example_prediction = clf.predict([X_test[example_idx]])\n","example_identity = encoder.inverse_transform(example_prediction)[0]\n","\n","plt.imshow(example_image)\n","plt.title(f'Identified as {example_identity}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r737oBIEeI0u"},"outputs":[],"source":["from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","from matplotlib import pyplot\n","ns_probs = [0 for _ in range(len(y_test))]\n","# fit a model\n","# model = LogisticRegression(solver='lbfgs')\n","# model.fit(x_train, trainy)\n","# predict probabilities\n","lr_probs = clf.predict_proba(X_test)\n","# keep probabilities for the positive outcome only\n","lr_probs = lr_probs[:, 1]\n","# calculate scores\n","ns_auc = roc_auc_score(y_test, ns_probs)\n","lr_auc = roc_auc_score(y_test, lr_probs)\n","# summarize scores\n","print('No Skill: ROC AUC=%.3f' % (ns_auc))\n","print('Logistic: ROC AUC=%.3f' % (lr_auc))\n","# calculate roc curves\n","ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n","lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n","# plot the roc curve for the model\n","pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n","pyplot.plot(lr_fpr, lr_tpr, marker='.', label='SVM')\n","# axis labels\n","pyplot.xlabel('False Positive Rate')\n","pyplot.ylabel('True Positive Rate')\n","# show the legend\n","pyplot.legend()\n","# show the plot\n","pyplot.show()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Face recognition.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}